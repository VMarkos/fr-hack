{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to build ML techniques to predict seven days of SST over the Mediterranean Sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of code to generate LSTM, CNN and RForest models to predict seven days of SST in 16 regions of the Mediterranean Sea. ML techniques are trained using 1981-2016 as the training period and 2017-2021 as the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:47:52.412263Z",
     "start_time": "2021-11-08T09:47:40.903082Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are specific for CMCC supercomputer in order to support parallel computation of the code on the CMCC cluster. You can find information on how to use the Dask.distributed and dask_jobqueue libraries at https://distributed.dask.org/en/stable/ and https://jobqueue.dask.org/en/latest/install.html. In particular we are asking for 36 cores, each of which starts 9 worker processes, and 2 jobs (i.e. 72 cores and 18 workers). If you do not have a cluster to run the code, you can skip this part and go to the 'Load data' section. Inevitably, the code will slow down and you can fall into memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bill/Documents/Projects/french_hackathon/venv/lib/python3.11/site-packages/dask_jobqueue/core.py:266: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask\n",
    "import dask.distributed\n",
    "from dask.distributed import Client\n",
    "\n",
    "from dask_jobqueue import LSFCluster\n",
    "cluster = LSFCluster(\n",
    "    queue='p_medium',\n",
    "    cores=36,\n",
    "    processes=9,\n",
    "    memory=\"85 GB\",\n",
    "    project=\"0456\",\n",
    "    walltime=\"04:00\",\n",
    "    job_extra=['-x'],\n",
    "    #use_stdin=False,\n",
    "    local_directory=os.environ['HOME']+\"/dask-worker-space\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "                cols.append(df.shift(i))\n",
    "                names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "                cols.append(df.shift(-i))\n",
    "                if i == 0:\n",
    "                        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "                else:\n",
    "                        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "                agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, In_test, En_test, n_lag, n_seq, exp):\n",
    "        # extract raw values\n",
    "        raw_values = series\n",
    "        raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "        # transform into supervised learning problem X, y\n",
    "        supervised = series_to_supervised(raw_values, n_lag, n_seq)\n",
    "        supervised_values = supervised.values\n",
    "        # split into train and test sets\n",
    "        if exp=='Q' or exp=='REXP':\n",
    "                train = supervised_values[:In_test]\n",
    "        elif exp=='A':\n",
    "                train = supervised_values[En_test:]\n",
    "        else:\n",
    "                train1=supervised_values[:In_test]\n",
    "                train2=supervised_values[En_test:]\n",
    "                train=np.concatenate((train1,train2),0)\n",
    "        test = supervised_values[In_test:En_test]\n",
    "        return train, test\n",
    "\n",
    "\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "        a=list()\n",
    "        for i in range(n_seq):\n",
    "                actual = [row[i] for row in test]\n",
    "                predicted = [forecast[i] for forecast in forecasts]\n",
    "                rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "                print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "                a.append(rmse)\n",
    "        return a\n",
    "    \n",
    "def mysel_daily(ds):\n",
    "    ds = ds.isel(lat=slice(2390,2750), lon=slice(3490,4700))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract time-series for SST (ESA CCI SST) and predictors (ERA5). The SST dataset used in this study is the European Space Agency (ESA) Climate Change Initiative SST dataset v2.1 (Merchant et al., 2019) and it is freely available at the CEDA catalogue here:\n",
    "https://catalogue.ceda.ac.uk/uuid/62c0f97b1eac4e0197a674870afe1ee6) from September 1981 to December 2016 and in the\n",
    "Copernicus CDS here: https://cds.climate.copernicus.eu/cdsapp#!/dataset/satellite-seasurfacetemperature?tab=overview from January 2017 to December 2021. The relevant atmospheric variables are taken from European Centre for Medium-Range  Weather Forecasts (ECMWF) ERA5 dataset (Hersbach et al., 2020) at https://cds.climate.copernicus.eu/cdsapp#!/dataset/\n",
    "reanalysis-era5-single-levels?tab=overview as daily mean (see the manuscript for variables details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ESA CCI dataset\n",
    "ds = xr.open_mfdataset(\"./*-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.nc\", combine='by_coords', parallel=True,preprocess=mysel_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####define limits of MedFS regions \n",
    "name=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16']\n",
    "lat1=np.array([35, 34.8,34.8,    36.6,  41.2,  32,  36.6, 40,  42.5,30, 35.2, 33.5,30])\n",
    "lat2=np.array([39, 39.4,39.4,  41.2,44.8, 36.6,  40.6,  42.5,  46,  35.2, 41, 37,33.5])\n",
    "lon1=np.array([-9, -1, 5,  9.25,  9.25, 9.25, 16.4,14.8,12.2,  21, 21, 26.3, 26.3])\n",
    "lon2=np.array([-1, 5, 9.25, 15, 12.1,  15, 21,  21,  21, 26.3, 28, 33,33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save SST for areas\n",
    "for ii in range(0,len(name)):\n",
    "    ESA=ds.where((ds.lat>lat1[ii]) & (ds.lat<lat2[ii])).where((ds.lon>lon1[ii]) & (ds.lon<lon2[ii]))\n",
    "    savesst=prova.mean(\"lon\").mean(\"lat\") \n",
    "    np.save('./SST_'+name[ii]+'.npy',savesst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define ERA5 variables\n",
    "nameF=['SLP','GEO','WIND','SLP','SENS','LAT','INC']\n",
    "namevar=['msl','z','vel','msl','sh','lh','inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(0, len(nameF)):\n",
    "    ERA5=xr.open_mfdataset(\"./ERA5_\"+nameF[ii]+\"/*.nc\", combine='by_coords', drop_variables=['month'], parallel=True)\n",
    "    ERA5=ERA5.interp(longitude=ds.lon, latitude=ds.lat)\n",
    "    ERA5=ERA5.sel(time=slice(\"1981-09-01\", \"2021-12-31\"))\n",
    "    ERA5=ERA5.rename({'longitude': 'lon'})\n",
    "    ERA5=ERA5.rename({'latitude': 'lat'})\n",
    "    ERA5= xr.where(ds.mask[0,:,:]==1, ERA5[namevar[ii]], np.nan)\n",
    "    for xx in range(0,len(name)):\n",
    "        tosave=ERA5.where((ERA5.lat>lat1[xx]) & (ERA5.lat<lat2[xx])).where((ERA5.lon>lon1[xx]) & (ERA5.lon<lon2[xx]))\n",
    "        tosave=tosave.mean(\"lon\").mean(\"lat\")\n",
    "        np.save('./ERA5_'+nameF[ii]+'_'+name[xx]+'.npy',tosave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define experiment name (e.g. in this case Reference Experiments (REXP), and test time window indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "itest=12906 ##index of 1January2017\n",
    "ftest=14732 ##index of 31December2021\n",
    "ML=['LSTM','CNN', 'RF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region and for each ML technique, the following codes load the data, prepare the data, train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(0,len(name)):\n",
    "    print(name[ii])\n",
    "    for mm in range(0,len(ML)):\n",
    "            SST = np.load('./SST_'+name[ii]+'.npy')\n",
    "            SST=SST-273.15\n",
    "            #Wind speed at 10m\n",
    "            WIND = np.load('./ERA5_WIND_'+name[ii]+'.npy')\n",
    "            #Geopotential height at 500hPa\n",
    "            GEO = np.load('./ERA5_GEO_'+name[ii]+'.npy')\n",
    "            #Mean Sea Level pressure\n",
    "            SLP = np.load('./ERA5_SLP_'+name[ii]+'MFS.npy')\n",
    "            #latent heat flux\n",
    "            LAT = np.load('./ERA5_LAT_'+name[ii]+'.npy')\n",
    "            #Sensible heat\n",
    "            SENS = np.load('./ERA5_SENS_'+name[ii]+'.npy')\n",
    "            #Incoming Solar Radiation \n",
    "            INC = np.load('./ERA5_INC_'+name[ii]+'MFS.npy')\n",
    "            \n",
    "            ##binary values to identify seasonality\n",
    "            time=pd.date_range(start=\"1981-09-01\",end=\"2021-12-31\",freq='D')\n",
    "            MM=np.array(time.month).astype('float64')\n",
    "\n",
    "            n_lag = 7\n",
    "            n_seq = 7\n",
    "            ES= ftest\n",
    "            IS = itest\n",
    "            \n",
    "            \n",
    "            # Normalize the data\n",
    "            scaler = MinMaxScaler()\n",
    "            WIND =WIND.reshape(-1, 1)\n",
    "            WIND = scaler.fit_transform(WIND)\n",
    "            SLP =SLP.reshape(-1, 1)\n",
    "            SLP = scaler.fit_transform(SLP)\n",
    "            LAT =LAT.reshape(-1, 1)\n",
    "            LAT = scaler.fit_transform(LAT)\n",
    "            SENS =SENS.reshape(-1, 1)\n",
    "            SENS = scaler.fit_transform(SENS)\n",
    "            INC =INC.reshape(-1, 1)\n",
    "            INC = scaler.fit_transform(INC)\n",
    "            GEO =GEO.reshape(-1, 1)\n",
    "            GEO = scaler.fit_transform(GEO)\n",
    "            SST =SST.reshape(-1, 1)\n",
    "            SST = scaler.fit_transform(SST)\n",
    "            sst_min = scaler.data_min_[0]\n",
    "            sst_max = scaler.data_max_[0]\n",
    "            \n",
    "            # prepare data\n",
    "            train, test = prepare_data(SST, IS, ES, n_lag, n_seq, exp)\n",
    "            trainW, testW=prepare_data(WIND, IS, ES, n_lag, n_seq, exp)\n",
    "            trainSLP, testSLP=prepare_data(SLP, IS, ES, n_lag, n_seq, exp)\n",
    "            trainHEAT, testHEAT=prepare_data(LAT, IS, ES, n_lag, n_seq, exp)\n",
    "            trainHEATS, testHEATS=prepare_data(SENS, IS, ES, n_lag, n_seq, exp)\n",
    "            trainHEATINC, testHEATINC=prepare_data(INC, IS, ES, n_lag, n_seq, exp)\n",
    "            trainGEO, testGEO=prepare_data(GEO, IS, ES, n_lag, n_seq, exp)\n",
    "            trainMM, testMM=prepare_data(MM, IS, ES,n_lag, n_seq, exp)\n",
    "            \n",
    "            if ML[mm]=='LSTM':\n",
    "                ###reorder data\n",
    "                X = np.stack((train[:, 0:n_lag], trainW[:, 0:n_lag],trainHEAT[:, 0:n_lag], trainHEATS[:, 0:n_lag], trainHEATINC[:, 0:n_lag], trainSLP[:, 0:n_lag], trainGEO[:, 0:n_lag],trainMM[:, 0:n_lag]),2)\n",
    "                y=train[:,n_lag:]\n",
    "                X_test = np.stack((test[:, 0:n_lag], testW[:, 0:n_lag],testHEAT[:, 0:n_lag], testHEATS[:, 0:n_lag], testHEATINC[:, 0:n_lag],testSLP[:, 0:n_lag],testGEO[:, 0:n_lag],testMM[:, 0:n_lag]),2)\n",
    "                y_test=test[:,n_lag:]\n",
    "            \n",
    "                model = Sequential()\n",
    "                model.add(LSTM(60, input_shape=(X.shape[1], X.shape[2])))\n",
    "                model.add(Dense(n_seq,activation='linear'))\n",
    "                model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "                history = model.fit(X, y, epochs=200, batch_size=150, validation_split=0.3, verbose=1, shuffle=False)\n",
    "                yhat = model.predict(X_test)\n",
    "                yhat = yhat * (sst_max - sst_min) + sst_min\n",
    "                y_test= y_test* (sst_max - sst_min) + sst_min\n",
    "                a=evaluate_forecasts(y_test, list(yhat), n_lag, n_seq)\n",
    "\n",
    "            if ML[mm]=='CNN':\n",
    "                \n",
    "                X = np.stack((train[:, 0:n_lag], trainW[:, 0:n_lag],trainHEAT[:, 0:n_lag], trainHEATS[:, 0:n_lag], trainHEATINC[:, 0:n_lag], trainSLP[:, 0:n_lag], trainGEO[:, 0:n_lag],trainMM[:, 0:n_lag]),2)\n",
    "                y=train[:,n_lag:]\n",
    "                X_test = np.stack((test[:, 0:n_lag], testW[:, 0:n_lag],testHEAT[:, 0:n_lag], testHEATS[:, 0:n_lag], testHEATINC[:, 0:n_lag],testSLP[:, 0:n_lag],testGEO[:, 0:n_lag],testMM[:, 0:n_lag]),2)\n",
    "                y_test=test[:,n_lag:]\n",
    "                \n",
    "                #model\n",
    "                model = Sequential()\n",
    "                model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "                model.add(MaxPooling1D(pool_size=2))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(y.shape[1]))\n",
    "                model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "                # fit network\n",
    "                history = model.fit(X, y, epochs=200, batch_size=150, validation_split=0.3, verbose=1, shuffle=False)\n",
    "                yhat = model.predict(X_test)\n",
    "                yhat = yhat * (sst_max - sst_min) + sst_min\n",
    "                y_test= y_test* (sst_max - sst_min) + sst_min\n",
    "                a=evaluate_forecasts(y_test, list(yhat), n_lag, n_seq)\n",
    "                \n",
    "                \n",
    "            if ML[mm]=='RF':\n",
    "                ###reorder data\n",
    "                X = np.stack((train[:, 0:n_lag], trainW[:, 0:n_lag],trainHEAT[:, 0:n_lag], trainHEATS[:, 0:n_lag], trainHEATINC[:, 0:n_lag], trainSLP[:, 0:n_lag], trainGEO[:, 0:n_lag],trainMM[:, 0:n_lag]),2)\n",
    "                y=train[:,n_lag:]\n",
    "                X_test = np.stack((test[:, 0:n_lag], testW[:, 0:n_lag],testHEAT[:, 0:n_lag], testHEATS[:, 0:n_lag], testHEATINC[:, 0:n_lag],testSLP[:, 0:n_lag],testGEO[:, 0:n_lag],testMM[:, 0:n_lag]),2)\n",
    "                y_test=test[:,n_lag:]\n",
    "                X_tr=X.reshape([y.shape[0],X.shape[1] * X.shape[2]])\n",
    "                Y_tr=y\n",
    "                X_ts=X_test.reshape([y_test.shape[0],X.shape[1] * X.shape[2]])\n",
    "\n",
    "        \n",
    "                # Instantiate model with 1000 decision trees\n",
    "                rf = RandomForestRegressor(n_estimators = 100, random_state = 42, verbose=1)\n",
    "                rf.fit(X_tr, Y_tr)\n",
    "                yhat = rf.predict(X_ts)\n",
    "                yhat = yhat * (sst_max - sst_min) + sst_min\n",
    "                y_test= y_test* (sst_max - sst_min) + sst_min\n",
    "                a=evaluate_forecasts(y_test, list(yhat), n_lag, n_seq)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
